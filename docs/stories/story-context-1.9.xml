<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>1</epicId>
    <storyId>1.9</storyId>
    <title>Validation Testing</title>
    <status>Draft</status>
    <generatedAt>2025-10-24</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/story-1.9.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>QA engineer and project stakeholder</asA>
    <iWant>comprehensive Stage 1-3 validation testing of the camera movement detection system</iWant>
    <soThat>we can verify >95% detection accuracy, zero false negatives, and <5% false positive rate before production deployment</soThat>
    <tasks>
      <task id="1" ac="1.9.1, 1.9.4">Stage 1 Test Harness Implementation (5 subtasks)</task>
      <task id="2" ac="1.9.1, 1.9.5">Stage 1 Test Data Preparation (5 subtasks)</task>
      <task id="3" ac="1.9.1">Execute Stage 1 Validation (5 subtasks)</task>
      <task id="4" ac="1.9.2, 1.9.5">Stage 2 Test Data Preparation (5 subtasks)</task>
      <task id="5" ac="1.9.2">Execute Stage 2 Validation (4 subtasks)</task>
      <task id="6" ac="1.9.3, 1.9.5">Stage 3 Pilot Site Preparation (5 subtasks)</task>
      <task id="7" ac="1.9.3">Execute Stage 3 Live Monitoring (5 subtasks)</task>
      <task id="8" ac="1.9.6">Results Analysis and Documentation (5 subtasks)</task>
      <task id="9" ac="1.9.7">Go/No-Go Decision and Recommendations (5 subtasks)</task>
    </tasks>
  </story>

  <acceptanceCriteria>
    <ac id="1.9.1">Stage 1 Validation - Simulated Transforms: Achieve >95% detection accuracy using 20-30 test images with known synthetic camera shifts (2px, 5px, 10px transformations)</ac>
    <ac id="1.9.2">Stage 2 Validation - Real Footage: Achieve 100% detection rate (zero false negatives) using recorded footage with documented camera movements</ac>
    <ac id="1.9.3">Stage 3 Validation - Live Deployment: Achieve <5% false positive rate during 1-week continuous monitoring at pilot site with manual alert verification</ac>
    <ac id="1.9.4">Test Harness Implementation: Create automated test harness for Stage 1 validation that applies known transformations and measures accuracy</ac>
    <ac id="1.9.5">Test Data Preparation: Prepare and document test datasets for all three validation stages (synthetic shifts, real footage, pilot site images)</ac>
    <ac id="1.9.6">Results Documentation: Document validation results, metrics, and findings in comprehensive test report</ac>
    <ac id="1.9.7">Go/No-Go Decision: Provide clear go/no-go recommendation based on validation results meeting all acceptance criteria thresholds</ac>
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/tech-spec-epic-MVP-001.md</path>
        <title>Technical Specification: Camera Movement Detection Module</title>
        <section>Acceptance Criteria</section>
        <snippet>AC-001: Detection accuracy >95% for movements â‰¥2 pixels in Stage 1 testing (simulated transforms). AC-002: Zero false negatives in Stage 2 testing (recorded footage with known movements). AC-003: False positive rate <5% during Stage 3 testing (1-week live deployment monitoring).</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-MVP-001.md</path>
        <title>Technical Specification: Camera Movement Detection Module</title>
        <section>Test Strategy Summary</section>
        <snippet>Three-stage validation: Stage 1 (Simulated transforms, 20-30 images, >95% accuracy target), Stage 2 (Real recorded shifts, 0 false negatives), Stage 3 (Live deployment, 1 week continuous, <5% false positives). Go/No-Go Criteria: All 10 acceptance criteria must pass.</snippet>
      </doc>
      <doc>
        <path>docs/tech-spec-epic-MVP-001.md</path>
        <title>Technical Specification: Camera Movement Detection Module</title>
        <section>Test Data Requirements</section>
        <snippet>Stage 1: 20-30 test images with synthetic shifts (2px, 5px, 10px). Stage 2: Existing recordings where camera actually moved (if available). Stage 3: 1 pilot site, multiple times of day, various DAF operational states.</snippet>
      </doc>
      <doc>
        <path>docs/stories/story-1.8.md</path>
        <title>Story 1.8: Comprehensive Test Coverage & Integration Test Suite</title>
        <section>Dev Notes</section>
        <snippet>Existing test infrastructure: pytest, pytest-cov, unittest.mock. 191 existing unit tests across all components. Sample images available in sample_images/ directory (of_jerusalem, carmit, gad datasets).</snippet>
      </doc>
    </docs>

    <code>
      <artifact>
        <path>src/camera_movement_detector.py</path>
        <kind>module</kind>
        <symbol>CameraMovementDetector</symbol>
        <lines>1-200</lines>
        <reason>Main API to be validated; process_frame() is the primary method under test</reason>
      </artifact>
      <artifact>
        <path>src/movement_detector.py</path>
        <kind>module</kind>
        <symbol>MovementDetector</symbol>
        <lines>1-150</lines>
        <reason>Core detection logic implementing homography-based movement calculation; critical for accuracy validation</reason>
      </artifact>
      <artifact>
        <path>tests/test_camera_movement_detector.py</path>
        <kind>test</kind>
        <symbol>existing unit tests</symbol>
        <lines>1-500</lines>
        <reason>35 existing tests provide foundation; validation tests build on this infrastructure</reason>
      </artifact>
      <artifact>
        <path>tests/test_integration.py</path>
        <kind>test</kind>
        <symbol>integration tests</symbol>
        <lines>1-300</lines>
        <reason>Integration tests from Story 1.8; validation extends with Stage 1-3 tests</reason>
      </artifact>
      <artifact>
        <path>tests/test_e2e.py</path>
        <kind>test</kind>
        <symbol>E2E tests</symbol>
        <lines>1-250</lines>
        <reason>E2E tests using sample images; template for Stage 2 real footage validation</reason>
      </artifact>
      <artifact>
        <path>tests/conftest.py</path>
        <kind>fixture</kind>
        <symbol>pytest fixtures</symbol>
        <lines>1-100</lines>
        <reason>Shared fixtures for detector initialization, test images, config files</reason>
      </artifact>
    </code>

    <dependencies>
      <python>
        <package name="numpy" version=">=1.24.0,<2.0.0" />
        <package name="opencv-python" version=">=4.8.0,<5.0.0" />
        <package name="pytest" version=">=7.0.0" />
        <package name="pytest-cov" version=">=4.0.0" />
      </python>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint>Validation testing must follow Tech Spec AC-001, AC-002, AC-003 thresholds exactly (>95%, 0%, <5%)</constraint>
    <constraint>Stage 1 must use synthetic transformations (translate, rotate, scale) with known ground truth</constraint>
    <constraint>Stage 2 requires documented real footage with known camera movements or simulated scenarios</constraint>
    <constraint>Stage 3 requires 1-week continuous monitoring with manual verification of all INVALID alerts</constraint>
    <constraint>Test harness must calculate TP, TN, FP, FN, accuracy, precision, recall, F1-score</constraint>
    <constraint>Validation report must document methodology, results, findings, and clear go/no-go recommendation</constraint>
    <constraint>Reuse existing test infrastructure (pytest, fixtures, sample_images/) from Story 1.8</constraint>
    <constraint>Create validation/ directory structure separate from tests/ for validation-specific code</constraint>
  </constraints>

  <interfaces>
    <interface>
      <name>CameraMovementDetector.process_frame()</name>
      <kind>Python API</kind>
      <signature>process_frame(image_array: np.ndarray, frame_id: str = None) -> Dict[str, Any]</signature>
      <path>src/camera_movement_detector.py</path>
      <notes>Primary method under validation; returns {"status": "VALID"|"INVALID", "displacement": float, "confidence": float, "frame_id": str, "timestamp": str}</notes>
    </interface>
    <interface>
      <name>CameraMovementDetector.set_baseline()</name>
      <kind>Python API</kind>
      <signature>set_baseline(image_array: np.ndarray) -> None</signature>
      <path>src/camera_movement_detector.py</path>
      <notes>Required before validation testing; captures baseline features for comparison</notes>
    </interface>
    <interface>
      <name>Ground Truth Label Format</name>
      <kind>JSON Schema</kind>
      <signature>{"image_id": str, "baseline_image": str, "transformation": {...}, "expected_status": str, "expected_displacement_range": [float, float]}</signature>
      <path>validation/stage1_data/ground_truth.json</path>
      <notes>Label format for Stage 1 synthetic dataset</notes>
    </interface>
    <interface>
      <name>Detection Results Format</name>
      <kind>JSONL Schema</kind>
      <signature>{"timestamp": str, "frame_id": str, "status": str, "displacement": float, "confidence": float}</signature>
      <path>validation/stage3_logs/detection_results.jsonl</path>
      <notes>Log format for Stage 3 live monitoring</notes>
    </interface>
  </interfaces>

  <tests>
    <standards>
      Use pytest framework with pytest-cov for coverage. Validation tests organized in validation/ directory separate from unit tests. Stage 1 tests automated (test harness), Stage 2 semi-automated (manual footage selection, automated analysis), Stage 3 manual monitoring with automated logging. Generate comprehensive validation report with metrics, visualizations (confusion matrices), and go/no-go recommendation.
    </standards>
    <locations>
      <location>validation/stage1_test_harness.py</location>
      <location>validation/stage2_validator.py</location>
      <location>validation/stage3_monitor.py</location>
      <location>validation/stage1_data/</location>
      <location>validation/stage2_data/</location>
      <location>validation/stage3_logs/</location>
      <location>validation/validation_report.md</location>
    </locations>
    <ideas>
      <idea ac="1.9.1">Stage 1: Generate 20-30 baseline images from sample_images/, apply synthetic shifts (2px, 5px, 10px in 8 directions), run detector, calculate accuracy metrics, verify >95% threshold</idea>
      <idea ac="1.9.2">Stage 2: Use real footage or simulate camera movement sequences, document movements with timestamps, run detector, verify 0 false negatives (100% detection)</idea>
      <idea ac="1.9.3">Stage 3: Deploy at pilot site for 1 week, log all detections, manually verify each INVALID alert (TP vs FP), calculate FP rate, verify <5% threshold</idea>
      <idea ac="1.9.4">Test harness: Implement synthetic transformation functions (translate, rotate, scale), ground truth label management, accuracy calculation (TP/TN/FP/FN), metrics reporting</idea>
      <idea ac="1.9.5">Test data: Organize validation/stage{1,2,3}_data directories, create ground truth JSON files, document data generation methodology, reuse sample_images/ for baselines</idea>
      <idea ac="1.9.6">Results documentation: Create validation_report.md with methodology, Stage 1-3 results, metrics tables, confusion matrices, failure analysis, findings summary</idea>
      <idea ac="1.9.7">Go/No-Go: Evaluate all AC thresholds, identify gaps/concerns, provide clear recommendation, document production deployment readiness or iteration requirements</idea>
    </ideas>
  </tests>
</story-context>
